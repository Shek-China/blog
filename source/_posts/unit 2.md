---
title: 机器学习 第2章 模型评估与选择
mathjax: true
---

# 第2章 模型评估与选择

## 经验误差与过拟合
通常，我们将分类错误的样本数占样本总数的比例称为"错误率"(error rate)，即如果在m个样本中有a个样本分类错误，则：
**错误率**:$ E = a/m\times100\%$
**精度** :$A = (1 - a/m)\times100\%$

## 2.1 一些基本概念：

* **误差(error):** 学习器的实际预测输出与样本的真实输出之间的差异。
* **训练误差&经验误差(training error&empirical error):** 学习器在**训练集**上的误差。
* **泛化误差(generalization):**学习器在**新样本**上的误差。
* **过拟合&过配(overfitting):**学习器将训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质的现象。(过拟合是机器学习面临的关键障碍)
* **欠拟合&欠配(underfitting):**学习器对训练样本的一般性质尚未学习好的现象。

![image-20210206212405211](../images/unit%202/image-20210206212405211.png)


## 2.2 评估方法

我们有一个包含m个样例的数据集 $D = {(x_1,y_1),(x_2,y_2), ...,(x_m,y_m)}$，通过对$ D $进行适当的处理，从中产生训练集S和测试集T，下面是几种常用的做法。

### (1) 留出法 (Hold-out)

​		剑桥五杰直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后用T来评估其测试误差，作为对泛化误差的估计。

​		*以二分类任务为例，假定D包含了1000个样本，其中700个划分为S，300个划分为T，用S进行训练后，如果模型在T上又90个样本分类错误，那么其错误率为 $ (90/300)\times100\%=30\% $,相应的，精度为$1-30\%=70\%$。*

​		其中，需要注意到，训练/测试集的划分要尽可能保持**数据分布的一致性**，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中保持样本的类别比例相似，从采样(sampling)的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为"**分层采样**"(stratified sampling)。

​		*接着刚才的例子说，假设D中包含500个正例和500个反例，则分层采样得到的S应包含350个正例，350个反例，而T则包含150个正例和150个反例；*若S、T中的样本类别比例差别很大，则误差估计将由于训练/测试数据分布的差异而产生偏差。

​		另外，同样应该注意的是：即便在给定训练/测试集的样本比例后，仍存在多种划分方式对初始数据集D进行分割。*例如在上面的例子中，可以把D终得样本排序，然后把前350个正例放到训练集中，也可以把最后350个正例放到训练集中...显然这中划分方式是有问题的。*因此，**单次使用留出法得到的估计结果往往不够稳定可靠**，因此，在使用留出法时，一般要**采用若干次随即划分、重复进行实验评估后取平均值(又是也可计算标准差)作为留出法的评估结果**。

​		此外还有一个问题，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这会导致评估结果的不准确且这种误差是无可避免的(*从"偏差-方差"的角度理解：测试集小时，评估结果的方差较大；训练集小时，评估结果的偏差较大*)，对此，常见的作法是将大约2/3~3/5的样本用于训练，剩余样本用于测试。

### (2) 交叉验证法 (Cross validation)

​		**交叉验证法**先将数据集D划分未k个大小相似的互斥子集，即 $ D=D_1\cup D_2\cup ... \cup D_k,D_i\cap D_j = \oslash (i\ne j)$ 每个子集$D_i$都极可能保持数据分布的一致性，即D中通过**分层采样**得到。然后，每次用k - 1个自己的并集作为训练集，余下的那个子集作为测试集，从而获得k组训练/测试集，从而可以进行k次训练和测试，最终返回的是这k个测试结果的均值，为此，通常把交叉验证法称为"k折交叉验证"(k-fold cross validation)。k最常用的取值为10，下图给出10折交叉验证的示意图：

![image-20210206212428809](../images/unit%202/image-20210206212428809.png)

​		类似于留出法，将数据集D划分为k个子集同样存在多种划分方式，为了减小误差，k这交叉验证通常要随即使用不同的划分重复p次，最终的评估结果是这p次k折交叉验证结果的均值，常见的有"**10次10折交叉验证**"(它和100次留出法都进行了100次训练/测试)。

​		假定数据集D中包含了m个样本，若令k = m ，则得到了交叉验证法的一个特例：**留一法**(Leave-One-Out，简称**LOO**)。显然，留一法不受随机样本划分的影响，因为m个样本只有唯一的方式划分为m个子集，每个子集只包含一个样本。因此，留一法的评估结果往往被认为是比较准确的。但，其代价也是巨大的：在数据集比较大时，**训练m个模型的计算开销可能是非常恐怖的**。例如当训练一百万个样本时，则需要训练100万个模型，这还没考虑算法调参的因素，显然，这很离谱。而且，由**NFL定理**，我们也可以得到一个结论：**留一法未必永远比其他评估方法准确**。

### (3) 自助法 (Bootstrapping)

​		**自助法**是一种可以减小训练样本规模不同造成的影响，同时还能比较高效地执行实验估计的方法，它以自助采样法(bootstrap sampling)为基础，给定包含m个样本的数据集D，我们对其进行采样产生数据集**D'**，具体采样方法如下：

​		每次随机从D中挑选一个样本，将其拷贝放入**D'**，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采集到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集**D'**，这就是自助采样的结果。

​		显然，在这个过程中会有一部分样本在D'中多次出现，而另一部分样本不出现，经过计算可以得出样本在m次采样中始终不被采到的概率为 $\large  (1-\frac{1}{m})^m $，取极限可得：	

$$
\Large \lim_{m \to \infty}(1-\frac{1}{m})^m \to \frac{1}{e}\approx0.368\tag{2.1}
$$

由此，初始数据集D中约有36.8%的样本未出现在采样数据集D'中，于是我们可以将D'用作训练集，D\D'用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有总量的约1/3的、未在训练集中出现的样本用于测试，这种测试结果称为"**包外估计**"(out-of-bag estimate).

适用范围：

* 处理数据集较小、难以有效划分训练/测试集；

* 集成学习 (自助法能从初始数据集中产生多个不同的训练集);

缺陷：
自助法产生的数据集改变了初始数据集的分布，可能会引入估计偏差，因此，在初始数据量足够时，或是处理大批量数据时，留出法和交叉验证法更常用。


### 调参与最终模型

​		大部分学习算法都有些参数(parameter)需要设定，参数配置不同，学得模型的性能往往有显著差别。因此，在进行模型评估与选择时，不但要选择算法，还要对算法参数进行设定，这就是 "**参数调节**"(parameter tuning)。

​		调参和算法选择没有本质区别，但调参不可能对每种参数配置都训练出模型，因为学习算法的很多参数要在实数范围内取值。实际上，调参是件非常麻烦的事情，比如：假定算法有3个参数，每个参数都有5个候选值，这样对每一组训练/测试集就有 $ 5^3=125 $ 个参数需要考察，而在实际的工程中要调整的参数远不止3个(*神经网络内部的参数称为"**超参数**"*)，这个工程量显然无法接受(大型深度学习模型中的参数甚至有上百亿个)，而调参本身又对模型的泛化能力有着非常深远的影响。

​		通常，我们把学得模型在世纪时会用中遇到的数据称为测试数据，为了加以区分，模型评估与选择中用于评估测试的数据集通常称为"**验证集**"(validation set)。

#### 训练集&测试集&验证集 三者的关系

本部分转载自**知乎**：	https://zhuanlan.zhihu.com/p/48976706

​		我们将数据集划分为训练集和测试集，我们让模型在训练集上进行训练，然后**在测试集上来近似模型的泛化能力**。我们如果想要挑选不同的模型的话，可以让两个模型分别在训练集上训练，然后将两个训练好的模型分别在测试集上进行测试，由于我们把测试集上的误差近似近似为泛化误差，所以我们自然可以选择在测试集上误差小的模型作为最终我们要选择的泛化能力强的模型。

​		但是我们要做的不仅是不同的模型与模型之间的对比，很多时候我们需要对模型本身进行选择，假如我们有两个模型，线性模型和神经网b络模型，我们知道神经网络的泛化能力要比线性模型要强，我们选择了神经网络模型，但是神经网络中还有很多的需要人工进行选择的参数，比如神经网络的层数和每层神经网络的神经元个数以及正则化的一些参数等等，我们将这些参数称为***超参数。***这些参数不同选择对模型最终的效果也很重要，我们在开发模型的时候总是需要调节这些超参数。

​		现在我们需要调节这些超参数来使得模型泛化能力最强。我们使用测试集来作为泛化误差估计，而我们最终的目的就是选择泛化能力强的模型，那么我们可以直接通过模型在测试集上的误差来调节这些参数不就可以了。可能模型在测试集上的误差为0，但是你拿着这样的模型去部署到真实场景中去使用的话，效果可能会非常差。

​		这一现象叫做**信息泄露**。我们使用测试集作为泛化误差的近似，所以不到最后是不能将测试集的信息泄露出去的，就好比考试一样，我们平时做的题相当于训练集，测试集相当于最终的考试，我们通过最终的考试来检验我们最终的学习能力，将测试集信息泄露出去，相当于学生提前知道了考试题目，那最后再考这些提前知道的考试题目，当然代表不了什么，你在最后的考试中得再高的分数，也不能代表你学习能力强。而如果通过测试集来调节模型，相当于不仅知道了考试的题目，学生还都学会怎么做这些题了（因为我们肯定会人为的让模型在测试集上的误差最小，因为这是你调整超参数的目的），那再拿这些题考试的话，人人都有可能考满分，但是并没有起到检测学生学习能力的作用。原来我们通过测试集来近似泛化误差，也就是通过考试来检验学生的学习能力，但是由于信息泄露，此时的测试集即考试无任何意义，现实中可能学生的能力很差。所以，我们在学习的时候，老师会准备一些小测试来帮助我们查缺补漏，这些小测试也就是要说的验证集。我们通过验证集来作为调整模型的依据，这样不至于将测试集中的信息泄露。

​		也就是说我们将数据划分训练集、验证集和测试集。在训练集上训练模型，在验证集上评估模型，一旦找到的最佳的参数，就在测试集上最后测试一次，测试集上的误差作为泛化误差的近似。关于验证集的划分可以参考测试集的划分，其实都是一样的，这里不再赘述。




## 2.3 性能度量

对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是**性能度量**(performance measure)。

在预测任务中，给定样例集$ \large D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)} $，其中 $\large y_i$ 是示例 $\large x_i$ 的真实标记，要评估学习器 $\large f$ 的性能，就要把学习器预测结果$\large f(x)$与真是标记 $\large y$ 进行比较。

回归任务最常用的性能度量是"**均方误差**"(mean squared error)

$$
\Large E(f;D)=\frac{1}{m} \sum_{i=1}^m (f(x_i)-y_i)^2\tag{2.2}
$$

更一般的，对于数据分布 $\large \mathcal{D}$ 和概率密度函数 $\large p(·)$，均方差误差可描述为
$$
\Large E(f;\mathcal{D})=\int_{x\sim\mathcal{D}}{(f(x)-y)^2p(x)dx} \tag{2.3}
$$
下面主要介绍分类任务中常用的性能度量。

### 2.3.1 错误率与精度

对于样例集D，**分类错误率**定义为：
$$
\Large E(f;D)=\frac{1}{m}\sum_{i=1}^m \mathbb{I}(f(x_i)\ne y_i)\tag{2.4}
$$

**精度**定义为：
$$
\begin{align} 
\Large acc(f;D)&\Large=\frac{1}{m}\sum_{i=1}^m \mathbb{I}(f(x_i)=y_i)\tag{2.5}\\
 &\Large=1-E(f;D)
\end{align}
$$

更一般的，对于数据分布 $\large\mathcal{D}$ 和概率密度 $\large p(·)$ ，错误率与精度分别可以描述为：
$$
\Large E(f;\mathcal D) =\int_{x\sim\mathcal{D}}\mathbb{I}(f(x)\ne y)p(x)dx,\tag{2.6}\\
$$
$$
\begin{align}
\Large acc(f;\mathcal D)&\Large=\int_{x\sim \mathcal{D}}\mathbb I(f(x)=y)p(x)d(x)\tag{2.7}\\
&\Large=1-E(f;\mathcal D)
\end{align}
$$

### 2.3.2 查准率、查全率与F1

有时候，错误率和精度无法完全满足需求，因此要引入其他的性能度量：**查准率(precision)、查全率(recall)**
对于二分类问题，可以将阳历根据真是类别与机器学习预测类别的组合划分为以下四种情形：

* 真正例 (true positive) TP
* 假正例 (false positive) FP
* 真反例 (true negative) TN
* 假反例 (false negative) FN

![image-20210206190554377](../images/unit%202/image-20210206190554377.png)

​                                                                          *混淆矩阵(confusion matrix)*

**查准率：**$ \Large P=\frac{TP}{TP+FP} \tag{2.8}$

**查全率：**$ \Large R=\frac{TP}{TP+FN} \tag{2.9} $

![image-20210206195209710](../images/unit%202/image-20210206195209710.png)

若一个学习器的P-R曲线被另一个曲线完全包住，则可以断言后者优于前者，例如A显然优于C

若两个学习器的P-R曲线发生了交叉，如A和B，则难以一般性断言优劣，这时会用到如下几种方法：

#### (1)面积估算法
一种比较合理的判据时比较P-R曲线下面积的大小，它能一定程度上表征学习器在查准率和查全率上取得相对"双高"的比例，但大部分时候这种方法都不太好估算。

#### (2)"平衡点"法 (Break-Event Point)
平衡点法简称**BEP**，取**查准率P=查全率R**时的取值，如图可以据此方法推断学习器A优于B。(0.8>0.6x)

#### (3)F1度量

F1度量公式定义如下：
$$
\Large F1=\frac{2 \times P \times R}{P+R}=\frac{2\times TP}{样例总数+TP-TN}\tag{2.11}
$$
它本质上时基于T、P的调和平均(harmonic mean)定义的:
$$
\frac{1}{F1}=\frac{1}{2}\times (\frac{1}{P}+\frac{1}{R})
$$
由于F1度量中的P和R在不同情况下的重视程度不同，因此可以将F1度量写成更一般的形式：$\Large F_\beta$，其公式定义如下：
$$
\Large F_\beta=\frac {(1+\beta ^2)\times P \times R}{(\beta ^2 \times P)+R} \tag{2.11}
$$

* $\beta > 0$ 度量了查全率对查准率的相对重要性；
* $\beta >1$ 时**查全率**有更大影响;
* $\beta <1$ 时**查准率**有更大影响；

上述方法以单个二分类混淆矩阵为基础，而实际上我们更希望在n个二分类混淆矩阵上综合考察准确率和查全率，有如下两种方法：

#### (4) 宏(Macro)

这种做法是现在各混淆矩阵上分别计算出查准率和查全率，记为 $\large (P_1,R_1),(P_2,R_2),...,(P_n,R_n)$ ，

再计算均值，就可以得到：
$$
\Large \textbf{宏查准率}：\text{macro-P} = \frac{1}{n}\sum_{i=1}^n P_i, \tag{2.12}
$$

$$
\Large \textbf{宏查全率}： \text{macro-R}=\frac{1}{n} \sum_{i=1}^{n}R_i \tag{2.13}
$$

$$
\Large \textbf{宏}{\large F1}：\text{macro-F1}=\frac{2\times \text{macro-P}\times \text{macro-R}}{\text{macro-P}+\text{macro-R}} \tag{2.14}
$$

#### (5) 微(Micro)

这种做法先将各混淆矩阵的对应元素进行平均，得到各元素均值后再基于均值计算出如下：
$$
\Large \textbf{微查准率}：\text{micro-P}=\frac{\overline{TP}}{\overline{TP}+\overline{FP}} \tag{2.15}
$$

$$
\Large \textbf{微查全率}：\text{micro-R}=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}\tag{2.16}
$$

$$
\Large \textbf{微}\large F1： \text{micro-F1}=\frac{2\times \text{micro-P}\times \text{micro-R}}{\text{micro-P}+\text{micro-R}}\tag{2.17}
$$

### 2.3.3 ROC与AUC

#### (1) ROC

**ROC**全称"**Receiver Operating Characteristic**" (受试者工作特征) 曲线，最初用于军用信号分析技术，后被引入机器学习领域（1989）。

ROC曲线采用"**真正例率**"（**True Positive Rate**）为纵轴，"**假正例率**"（**False Positive Rate**）为横轴，两者分别定义为：
$$
\Large \text{TPR} = \frac{TP}{TP+FN} \tag{2.18}
$$

$$
\Large \text{FPR}=\frac{FP}{TN+FP}\tag{2.19}
$$

![image-20210210124946471](../images/unit%202/image-20210210124946471.png)

图(a)为理想状态下的ROC曲线，图(b)为现实中的ROC曲线，其绘制过程如下：

* 给定 $m^+$个正例以及 $m^-$个反例，根据学习器预测结果对样例进行排序

* 从(0,0)开始，设前一个标记点为(x,y)

* 若当前点为真正例，则当前点的坐标为 $(x,y+\frac{1}{m^+})$ 

* 若当前点为假正例，则当前点坐标为  $(x+\frac{1}{m^-},y)$ 

* 用线段将点依次连接

（书上讲的有点复杂了）

#### (2) AUC
AUC全称"Area Under ROC Curve"，即"ROC曲线下的区域"，类似于P-R图的比较方式，通过比较AUC的面积大小可以较为合理地判断不同学习器的优劣。

假定ROC曲线是由点集$\{ (x_1,y_1),(x_2,y_2),...,(x_m,y_m) \} $按顺序依次连接形成（参考图b），可以将AUC估算为：
$$
\Large \text{AUC}=\frac{1}{2}\sum_{i=1}^{m-1} (x_{i+1}-x_i)·(y_i+y_{i+1})\tag{2.20}
$$

AUC考虑的是样本预测的排序质量，因此它与排序误差有紧密联系。给定 $m^+$ 个正例和 $ m^- $个反例，令 $D^+$ 和 $D^-$ 分别表示正、反例集合，则可以将排序**"损失"(loss)**定义为：

$$
\large \ell_{rank} = \frac{1}{m+m^-}\sum_{x^+\in D^+}\sum_{x^-\in D^-}(\mathbb{I}(f(x_+)<f(x_-))+\frac{1}{2}\mathbb I (f(x^+)=f(x^-)))\tag{2.21}
$$

$\large \ell_{rank}$ 对应的是ROC曲线之上的面积，也就是白色部分，公式2.21可以理解为如果正例的预测值小于反例，则计一个"罚分"，相等计0.5个，最后将罚分总和相加计算均值。AUC与$\large \ell_{rank}$ 之间的 关系也很简单：
$$
\Large \text{AUC}=1-\ell_{rank} \tag{2.22}
$$


### 2.3.4 代价敏感错误率与代价曲线

以二分类任务为例，根据任务的领域知识设定一个"代价矩阵" (cost matrix),其中，$\large cost_{ij}$ 表示将第i类样本预测为第j类样本的代价，如下表：

<img src="../images/unit%202/image-20210210172544654.png" alt="image-20210210172544654" style="zoom:90%;" />

本方法与前面几种最大的不同就是**代价是否均等**，前面的方法都隐式地架设了均等代价，而在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化"总体代价" (total cost)。

将表2.2中的第0类作为正类、第1类作为范雷，$D^+$和$D^-$代表正/负例子集，可得"代价敏感" (cost-sensitive)错误率为：
$$
\large E(f;D;cost)=\frac{1}{m}(\sum_{x_i\in D^+}\mathbb I (f(x_i)\ne y_i)\times cost_{01}+\sum_{x_i \in D^- }\mathbb I (f(x_i)\ne y_i)\times cost_{10})\tag {2.23}
$$
在非均等代价下，ROC曲线无法直接反映出学习器的期望总体代价，取而代之的是"代价曲线" (cost curve)则可达到该目的。代价曲线图地横轴取值是[0,1]的**正例概率代价**：
$$
\large P(+)_{cost}=\frac{p \times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}} \tag{2.24}
$$
其中p是样例为正例的概率;纵轴是取值为[0,1]的**归一化代价**：
$$
\large cost_{norm}=\frac{FNR\times p \times cost_{01}+FPR \times (1-p)\times cost_{10}}{p \times cost_{01}+(1-p)\times cost_{10}}\tag{2.25}
$$
其中的**FPR**参考式(2.19)的**假正例率**，**FNR = 1 - TPR**是**假反例率**。设ROC曲线上点的坐标为(TPR,FPR)，即(真正例率，假正例率)，我们不难计算**假正例率FDR**和**假反例率FNR**，然后在代价平面上绘制一条从(0, FPR)到(1, FNR)的线段，线段下的面积就代表了**该条件下**的**期望总体代价**（可以理解成单个点的），如此将ROC上的每个点转化成代价平面上的一条线段，然后取所有线段的下界共同围成的面积，该面积即为在所有条件下学习器的期望总体**代价**。(如下图)

![image-20210210180127641](../images/unit%202/image-20210210180127641.png)

## 2.4 比较检验

