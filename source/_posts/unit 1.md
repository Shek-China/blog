---
title: 机器学习 第1章 绪论
---

# 第1章 绪论

## 基本术语

**分类(classification):**如果要预测的是离散值，例如"好瓜"和"坏瓜"，这类学习任务就成为"**分类**"。

**回归(regression):**如果要预测的是连续值，例如西瓜成熟度0.95、0.37，此类学习任务称为"**回归**"。

**聚类(clustering):**将训练集中的样本（西瓜）分为若干组，每组称为一个簇（cluster）；这些自动生成的簇可能对应一些潜在的概念划分，例如"浅色瓜"、"深色瓜"等。

根据训练数据是否拥有标记信息，学习任务大致可以划分为两大类："**监督学习**"（supervised learning）和"**无监督学习**"(unsupervised learning)，**分类**和**回归**是前者的代表，而**聚类**则是后者的代表。

**泛化能力(generalization ability):**模型对于未出现在训练集中的样本(新样本)的适应能力。

## 假设空间

**归纳推理(induction):**从特殊到一般的"泛化"过程，即从具体的事实归接触一般性规律。

**演绎推理(deduction):**从一般到特殊的"特化"(specialization)过程，即从基础原理推演出具体情况。

**归纳学习(inductive learning):**"从样例中学习"是一个归纳的过程，因此也称"归纳学习"，它有**广义**与**狭义**之分：广义的归纳学习大体相当于从样例中学习，而狭义的归纳学习则要求从训练数据中学得**概念(concept)**，因此亦称为"**概念学习**"或"**概念形成**"。(目前来说这玩意儿貌似挺难实现的)

**归纳偏好(inductive bias):**机器学习算法再学习过程中对某种模型假设的偏好称为"**归纳偏好**"，简称"**偏好**"。对此可以用下图来进行进一步的解释：

![image-20210209012432266](../images/%E7%AC%AC1%E7%AB%A0%20%E7%BB%AA%E8%AE%BA/image-20210209012432266.png)

不同的归纳偏好可能会产生如图所示的两种曲线A、B，它们都拥有相同的训练集，但最终由训练集归纳产生的模型却会因不同的归纳偏好而出现不同的结果。归纳偏好可以看作是学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或"价值观"。

**奥卡姆剃刀原则(Occam's Razor ):**若有多个假设与观察一致，则选最简单的那个。（当然，究竟什么是"简单"也是有一定的界定机制的) 

### NFL定理

**"没有免费的午餐"定理(No Free Lunch Theorem，简称NFL定理):**对于学习算法A和学习算法B，无论A多聪明B多笨拙，它们的期望性能相同。

<img src="../images/unit%201/image-20210209012449176.png" alt="image-20210209012449176" style="zoom:80%;" />

定理证明(这里偷个懒)：

<img src="../images/unit%201/image-20210209012513039.png" alt="image-20210209012513039" style="zoom:80%;" />

![image-20210209012530796](../images/unit%201/image-20210209012530796.png)

该定理有一个**重要前提**：所有"问题"出现的机会相同、或所有问题同等重要。

很多时候，我们只关注自己正在试图解决的问题，希望为其找到一个解决方案，至于这个方案在别的问题、甚至在相似问题上是否未好方案，我们并不关心。 NFL定理最重要的寓意，在于让我们清楚地认识到，脱离实际问题去空泛地谈论"什么学习算法更好"毫无意义。

